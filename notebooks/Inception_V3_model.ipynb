{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c98391-73d1-4380-8c07-f2c91f9312cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5d89da-9ac9-4edc-8f2f-d284b85c6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee82a9c-a62a-474d-89d9-276ce2053a30",
   "metadata": {},
   "source": [
    "# Read and split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d9cac9-cdce-47fc-a4ca-542c217eaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('last_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd53ea1f-21b9-40f8-afcd-536bbac6cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {'Pnemonia':1,\n",
    "                'Normal':0}\n",
    "df['Label'] = df['Label'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe976dc-657a-4fc8-8a59-a21992d9d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df.Dataset_type=='TRAIN']\n",
    "test_df = df[df.Dataset_type=='TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6323bac0-01cd-43ea-a451-5ef360b9be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pneumonia_df = train_df[train_df.Label==1]\n",
    "train_normal_df = train_df[train_df.Label==0]\n",
    "test_pneumonia_df = test_df[test_df.Label==1]\n",
    "test_normal_df = test_df[test_df.Label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024fa6f7-354d-414d-913e-55f17b25f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "num_train_p= len(train_pneumonia_df)\n",
    "num_train_n = len(train_normal_df)\n",
    "\n",
    "t_n_train = num_train_p+num_train_n\n",
    "\n",
    "#Take 10% from train to be validation\n",
    "\n",
    "nval_p = round(0.1*num_train_p)\n",
    "nval_n = round(0.1*num_train_n)\n",
    "\n",
    "print(nval_p)\n",
    "print(nval_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536cb306-4221-4888-82fe-5374e70b8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pneumonia_df = train_pneumonia_df[0:nval_p]\n",
    "train_pneumonia_df = train_pneumonia_df[nval_p:num_train_p]\n",
    "\n",
    "val_normal_df = train_normal_df[0:nval_n]\n",
    "train_normal_df = train_normal_df[nval_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf78a30b-7e30-4167-8c61-42caf12bf3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_images_pneumonia = val_pneumonia_df.X_ray_image_name.values.tolist()\n",
    "val_images_normal = val_normal_df.X_ray_image_name.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7842e8-b6f3-4249-a6b4-57e4cf15a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory():\n",
    "    try:\n",
    "        os.makedirs('Coronahack-Chest-XRay-Dataset/val/Pneumonia')\n",
    "        os.makedirs('Coronahack-Chest-XRay-Dataset/val/Normal')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "create_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a6834f-d2d3-420c-864a-a6a225d83642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='Coronahack-Chest-XRay-Dataset/train'\n",
    "test_path='Coronahack-Chest-XRay-Dataset/test'\n",
    "# for image in val_images_pneumonia:\n",
    "#     val_image_pneumonia = os.path.join(train_path, str(image))\n",
    "#     shutil.copy(val_image_pneumonia, 'Coronahack-Chest-XRay-Dataset/val/Pneumonia')\n",
    "    \n",
    "# for image in val_images_normal:\n",
    "#     val_image_normal = os.path.join(train_path, str(image))\n",
    "#     shutil.copy(val_image_normal, 'Coronahack-Chest-XRay-Dataset/val/Normal')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b524fba-6ffe-4bea-8d2a-ac5f26bd8e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4157 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_data_generation = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   rotation_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_data_generation.flow_from_directory('Coronahack-Chest-XRay-Dataset/train',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_generator = valid_datagen.flow_from_directory('Coronahack-Chest-XRay-Dataset/val',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93fc0240-fcfb-4272-a555-cb40d3b7f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 32, 32) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_1/beta:0' shape=(32,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_2/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_3/kernel:0' shape=(1, 1, 64, 80) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_3/beta:0' shape=(80,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 80, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_4/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_8/kernel:0' shape=(1, 1, 192, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_8/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 64, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_6/kernel:0' shape=(1, 1, 192, 48) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_6), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_9/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_7), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_6/beta:0' shape=(48,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_11/kernel:0' shape=(1, 1, 192, 32) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_10/kernel:0' shape=(3, 3, 96, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_7/kernel:0' shape=(5, 5, 48, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_5/kernel:0' shape=(1, 1, 192, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_8), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_11/beta:0' shape=(32,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_10/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_10), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_7/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_11), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_5/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_15/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_12), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_15/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_16/kernel:0' shape=(3, 3, 64, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_13/kernel:0' shape=(1, 1, 256, 48) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_13), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_16/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_14), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_13/beta:0' shape=(48,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_18/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_17/kernel:0' shape=(3, 3, 96, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_14/kernel:0' shape=(5, 5, 48, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_12/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_15), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_18/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_16), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_17/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_17), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_14/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_18), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_12/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_22/kernel:0' shape=(1, 1, 288, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_19), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_22/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_23/kernel:0' shape=(3, 3, 64, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_20/kernel:0' shape=(1, 1, 288, 48) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_20), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_23/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_21), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_20/beta:0' shape=(48,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_25/kernel:0' shape=(1, 1, 288, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 96, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_21/kernel:0' shape=(5, 5, 48, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_19/kernel:0' shape=(1, 1, 288, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_22), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_25/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_23), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_24/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_24), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_21/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_25), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_19/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_27/kernel:0' shape=(1, 1, 288, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_26), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_27/beta:0' shape=(64,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_28/kernel:0' shape=(3, 3, 64, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_27), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_28/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_29/kernel:0' shape=(3, 3, 96, 96) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_26/kernel:0' shape=(3, 3, 288, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_28), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_29/beta:0' shape=(96,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_29), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_26/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_34/kernel:0' shape=(1, 1, 768, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_30), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_34/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_35/kernel:0' shape=(7, 1, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_31), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_35/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_32), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_36/kernel:0' shape=(1, 7, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_33), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_31/kernel:0' shape=(1, 1, 768, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_32), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_36/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_33), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_31/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_34), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_37/kernel:0' shape=(7, 1, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_35), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_32/kernel:0' shape=(1, 7, 128, 128) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_34), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_37/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_35), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_32/beta:0' shape=(128,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_36), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_39/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_37), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_38/kernel:0' shape=(1, 7, 128, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_38), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_33/kernel:0' shape=(7, 1, 128, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_39), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_30/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_36), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_39/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_37), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_38/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_38), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_33/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_39), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_30/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_40), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_44/kernel:0' shape=(1, 1, 768, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_40), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_44/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_41), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_45/kernel:0' shape=(7, 1, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_41), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_45/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_42), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_46/kernel:0' shape=(1, 7, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_43), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_41/kernel:0' shape=(1, 1, 768, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_42), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_46/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_43), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_41/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_44), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_47/kernel:0' shape=(7, 1, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_45), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_42/kernel:0' shape=(1, 7, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_44), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_47/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_45), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_42/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_46), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_49/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_47), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_48/kernel:0' shape=(1, 7, 160, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_48), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_43/kernel:0' shape=(7, 1, 160, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_49), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_40/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_46), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_49/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_47), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_48/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_48), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_43/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_49), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_40/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_50), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_54/kernel:0' shape=(1, 1, 768, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_50), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_54/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_51), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_55/kernel:0' shape=(7, 1, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_51), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_55/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_52), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_56/kernel:0' shape=(1, 7, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_53), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_51/kernel:0' shape=(1, 1, 768, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_52), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_56/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_53), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_51/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_54), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_57/kernel:0' shape=(7, 1, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_55), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_52/kernel:0' shape=(1, 7, 160, 160) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_54), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_57/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_55), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_52/beta:0' shape=(160,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_56), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_59/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_57), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_58/kernel:0' shape=(1, 7, 160, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_58), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_53/kernel:0' shape=(7, 1, 160, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_59), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_50/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_56), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_59/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_57), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_58/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_58), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_53/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_59), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_50/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_60), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_64/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_60), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_64/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_61), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_65/kernel:0' shape=(7, 1, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_61), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_65/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_62), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_66/kernel:0' shape=(1, 7, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_63), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_61/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_62), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_66/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_63), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_61/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_64), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_67/kernel:0' shape=(7, 1, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_65), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_62/kernel:0' shape=(1, 7, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_64), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_67/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_65), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_62/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_66), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_69/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_67), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_68/kernel:0' shape=(1, 7, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_68), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_63/kernel:0' shape=(7, 1, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_69), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_60/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_66), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_69/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_67), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_68/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_68), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_63/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_69), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_60/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_70), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_72/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_70), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_72/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_71), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_73/kernel:0' shape=(1, 7, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_71), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_73/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_72), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_74/kernel:0' shape=(7, 1, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_73), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_70/kernel:0' shape=(1, 1, 768, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_72), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_74/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_73), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_70/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_74), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_75/kernel:0' shape=(3, 3, 192, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_75), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_71/kernel:0' shape=(3, 3, 192, 320) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_74), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_75/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_75), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_71/beta:0' shape=(320,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_76), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_80/kernel:0' shape=(1, 1, 1280, 448) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_76), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_80/beta:0' shape=(448,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_77), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_81/kernel:0' shape=(3, 3, 448, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_78), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_77/kernel:0' shape=(1, 1, 1280, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_77), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_81/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_78), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_77/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_79), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_83/kernel:0' shape=(3, 1, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_80), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_82/kernel:0' shape=(1, 3, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_81), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_79/kernel:0' shape=(3, 1, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_82), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_78/kernel:0' shape=(1, 3, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_83), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_84/kernel:0' shape=(1, 1, 1280, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_79), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_83/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_80), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_82/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_81), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_79/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_82), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_78/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_84), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_76/kernel:0' shape=(1, 1, 1280, 320) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_83), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_84/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_84), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_76/beta:0' shape=(320,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_85), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_89/kernel:0' shape=(1, 1, 2048, 448) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_85), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_89/beta:0' shape=(448,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_86), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_90/kernel:0' shape=(3, 3, 448, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_87), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_86/kernel:0' shape=(1, 1, 2048, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_86), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_90/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_87), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_86/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_88), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_92/kernel:0' shape=(3, 1, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_89), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_91/kernel:0' shape=(1, 3, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_90), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_88/kernel:0' shape=(3, 1, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_91), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_87/kernel:0' shape=(1, 3, 384, 384) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_92), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_93/kernel:0' shape=(1, 1, 2048, 192) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_88), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_92/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_89), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_91/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_90), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_88/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_91), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_87/beta:0' shape=(384,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.convolution_93), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'conv2d_85/kernel:0' shape=(1, 1, 2048, 320) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_92), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_93/beta:0' shape=(192,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_93), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'batch_normalization_85/beta:0' shape=(320,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "#Creating the model\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', pooling='max', input_shape=(224,224,3))\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation = \"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation = \"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs,outputs)\n",
    "\n",
    "base_model.trainable = False\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f094d7f-1752-4abd-81ed-b48c3c0c79a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution (TFOpLambda)  (None, 111, 111, 32) 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 111, 111, 32 0           tf.nn.convolution[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu (TFOpLambda)         (None, 111, 111, 32) 0           tf.compat.v1.nn.fused_batch_norm[\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_1 (TFOpLambda (None, 109, 109, 32) 0           tf.nn.relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 109, 109, 32 0           tf.nn.convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_1 (TFOpLambda)       (None, 109, 109, 32) 0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_2 (TFOpLambda (None, 109, 109, 64) 0           tf.nn.relu_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 109, 109, 64 0           tf.nn.convolution_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_2 (TFOpLambda)       (None, 109, 109, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.max_pool (TFOpL (None, 54, 54, 64)   0           tf.nn.relu_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_3 (TFOpLambda (None, 54, 54, 80)   0           tf.compat.v1.nn.max_pool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 54, 54, 80), 0           tf.nn.convolution_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_3 (TFOpLambda)       (None, 54, 54, 80)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_4 (TFOpLambda (None, 52, 52, 192)  0           tf.nn.relu_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 52, 52, 192) 0           tf.nn.convolution_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_4 (TFOpLambda)       (None, 52, 52, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.max_pool_1 (TFO (None, 25, 25, 192)  0           tf.nn.relu_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_5 (TFOpLambda (None, 25, 25, 64)   0           tf.compat.v1.nn.max_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_5 (TFOpLambda)       (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_7 (TFOpLambda (None, 25, 25, 48)   0           tf.compat.v1.nn.max_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_6 (TFOpLambda (None, 25, 25, 96)   0           tf.nn.relu_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 48), 0           tf.nn.convolution_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_7 (TFOpLambda)       (None, 25, 25, 48)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_6 (TFOpLambda)       (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool (TFOpL (None, 25, 25, 192)  0           tf.compat.v1.nn.max_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_11 (TFOpLambd (None, 25, 25, 64)   0           tf.compat.v1.nn.max_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_10 (TFOpLambd (None, 25, 25, 64)   0           tf.nn.relu_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_9 (TFOpLambda (None, 25, 25, 96)   0           tf.nn.relu_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_8 (TFOpLambda (None, 25, 25, 32)   0           tf.compat.v1.nn.avg_pool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 32), 0           tf.nn.convolution_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_8 (TFOpLambda)       (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_9 (TFOpLambda)       (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_10 (TFOpLambda)      (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_11 (TFOpLambda)      (None, 25, 25, 32)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 25, 25, 256)  0           tf.nn.relu_8[0][0]               \n",
      "                                                                 tf.nn.relu_9[0][0]               \n",
      "                                                                 tf.nn.relu_10[0][0]              \n",
      "                                                                 tf.nn.relu_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_12 (TFOpLambd (None, 25, 25, 64)   0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_12 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_14 (TFOpLambd (None, 25, 25, 48)   0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_13 (TFOpLambd (None, 25, 25, 96)   0           tf.nn.relu_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 48), 0           tf.nn.convolution_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_14 (TFOpLambda)      (None, 25, 25, 48)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_13 (TFOpLambda)      (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_1 (TFO (None, 25, 25, 256)  0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_18 (TFOpLambd (None, 25, 25, 64)   0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_17 (TFOpLambd (None, 25, 25, 64)   0           tf.nn.relu_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_16 (TFOpLambd (None, 25, 25, 96)   0           tf.nn.relu_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_15 (TFOpLambd (None, 25, 25, 64)   0           tf.compat.v1.nn.avg_pool_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_15 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_16 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_17 (TFOpLambda)      (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_18 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, 25, 25, 288)  0           tf.nn.relu_15[0][0]              \n",
      "                                                                 tf.nn.relu_16[0][0]              \n",
      "                                                                 tf.nn.relu_17[0][0]              \n",
      "                                                                 tf.nn.relu_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_19 (TFOpLambd (None, 25, 25, 64)   0           tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_19 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_21 (TFOpLambd (None, 25, 25, 48)   0           tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_20 (TFOpLambd (None, 25, 25, 96)   0           tf.nn.relu_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 48), 0           tf.nn.convolution_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_21 (TFOpLambda)      (None, 25, 25, 48)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_20 (TFOpLambda)      (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_2 (TFO (None, 25, 25, 288)  0           tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_25 (TFOpLambd (None, 25, 25, 64)   0           tf.concat_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_24 (TFOpLambd (None, 25, 25, 64)   0           tf.nn.relu_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_23 (TFOpLambd (None, 25, 25, 96)   0           tf.nn.relu_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_22 (TFOpLambd (None, 25, 25, 64)   0           tf.compat.v1.nn.avg_pool_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_22 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_23 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_24 (TFOpLambda)      (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_25 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 25, 25, 288)  0           tf.nn.relu_22[0][0]              \n",
      "                                                                 tf.nn.relu_23[0][0]              \n",
      "                                                                 tf.nn.relu_24[0][0]              \n",
      "                                                                 tf.nn.relu_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_26 (TFOpLambd (None, 25, 25, 64)   0           tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 64), 0           tf.nn.convolution_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_26 (TFOpLambda)      (None, 25, 25, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_27 (TFOpLambd (None, 25, 25, 96)   0           tf.nn.relu_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 25, 25, 96), 0           tf.nn.convolution_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_27 (TFOpLambda)      (None, 25, 25, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_29 (TFOpLambd (None, 12, 12, 384)  0           tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_28 (TFOpLambd (None, 12, 12, 96)   0           tf.nn.relu_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 384) 0           tf.nn.convolution_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 96), 0           tf.nn.convolution_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_28 (TFOpLambda)      (None, 12, 12, 384)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_29 (TFOpLambda)      (None, 12, 12, 96)   0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.max_pool_2 (TFO (None, 12, 12, 288)  0           tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_3 (TFOpLambda)        (None, 12, 12, 768)  0           tf.nn.relu_28[0][0]              \n",
      "                                                                 tf.nn.relu_29[0][0]              \n",
      "                                                                 tf.compat.v1.nn.max_pool_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_30 (TFOpLambd (None, 12, 12, 128)  0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.convolution_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_30 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_31 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.convolution_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_31 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_33 (TFOpLambd (None, 12, 12, 128)  0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_32 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.convolution_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.convolution_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_33 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_32 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_35 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_34 (TFOpLambd (None, 12, 12, 128)  0           tf.nn.relu_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.convolution_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 128) 0           tf.nn.convolution_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_35 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_34 (TFOpLambda)      (None, 12, 12, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_3 (TFO (None, 12, 12, 768)  0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_39 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_38 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_37 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_36 (TFOpLambd (None, 12, 12, 192)  0           tf.compat.v1.nn.avg_pool_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_36 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_37 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_38 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_39 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_4 (TFOpLambda)        (None, 12, 12, 768)  0           tf.nn.relu_36[0][0]              \n",
      "                                                                 tf.nn.relu_37[0][0]              \n",
      "                                                                 tf.nn.relu_38[0][0]              \n",
      "                                                                 tf.nn.relu_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_40 (TFOpLambd (None, 12, 12, 160)  0           tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_40 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_41 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_41 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_43 (TFOpLambd (None, 12, 12, 160)  0           tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_42 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_43 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_42 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_45 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_44 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_45 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_44 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_4 (TFO (None, 12, 12, 768)  0           tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_49 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_48 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_47 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_46 (TFOpLambd (None, 12, 12, 192)  0           tf.compat.v1.nn.avg_pool_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_49[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_48[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_47[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_46[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_46 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_47 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_48 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_49 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_5 (TFOpLambda)        (None, 12, 12, 768)  0           tf.nn.relu_46[0][0]              \n",
      "                                                                 tf.nn.relu_47[0][0]              \n",
      "                                                                 tf.nn.relu_48[0][0]              \n",
      "                                                                 tf.nn.relu_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_50 (TFOpLambd (None, 12, 12, 160)  0           tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_50[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_50 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_51 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_51[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_51 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_53 (TFOpLambd (None, 12, 12, 160)  0           tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_52 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_53[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_52[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_53 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_52 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_55 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_54 (TFOpLambd (None, 12, 12, 160)  0           tf.nn.relu_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_55[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 160) 0           tf.nn.convolution_54[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_55 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_54 (TFOpLambda)      (None, 12, 12, 160)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_5 (TFO (None, 12, 12, 768)  0           tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_59 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_58 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_57 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_56 (TFOpLambd (None, 12, 12, 192)  0           tf.compat.v1.nn.avg_pool_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_59[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_58[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_57[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_56[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_56 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_57 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_58 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_59 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_6 (TFOpLambda)        (None, 12, 12, 768)  0           tf.nn.relu_56[0][0]              \n",
      "                                                                 tf.nn.relu_57[0][0]              \n",
      "                                                                 tf.nn.relu_58[0][0]              \n",
      "                                                                 tf.nn.relu_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_60 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_60[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_60 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_61 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_61[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_61 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_63 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_62 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_63[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_62[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_63 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_62 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_65 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_64 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_65[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_64[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_65 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_64 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_6 (TFO (None, 12, 12, 768)  0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_69 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_68 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_67 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_66 (TFOpLambd (None, 12, 12, 192)  0           tf.compat.v1.nn.avg_pool_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_69[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_68[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_67[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_66[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_66 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_67 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_68 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_69 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_7 (TFOpLambda)        (None, 12, 12, 768)  0           tf.nn.relu_66[0][0]              \n",
      "                                                                 tf.nn.relu_67[0][0]              \n",
      "                                                                 tf.nn.relu_68[0][0]              \n",
      "                                                                 tf.nn.relu_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_70 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_70[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_70 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_71 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_71[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_71 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_73 (TFOpLambd (None, 12, 12, 192)  0           tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_72 (TFOpLambd (None, 12, 12, 192)  0           tf.nn.relu_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_73[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 12, 12, 192) 0           tf.nn.convolution_72[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_73 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_72 (TFOpLambda)      (None, 12, 12, 192)  0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_75 (TFOpLambd (None, 5, 5, 320)    0           tf.nn.relu_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_74 (TFOpLambd (None, 5, 5, 192)    0           tf.nn.relu_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 320),  0           tf.nn.convolution_75[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 192),  0           tf.nn.convolution_74[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_74 (TFOpLambda)      (None, 5, 5, 320)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_75 (TFOpLambda)      (None, 5, 5, 192)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.max_pool_3 (TFO (None, 5, 5, 768)    0           tf.concat_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_8 (TFOpLambda)        (None, 5, 5, 1280)   0           tf.nn.relu_74[0][0]              \n",
      "                                                                 tf.nn.relu_75[0][0]              \n",
      "                                                                 tf.compat.v1.nn.max_pool_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_76 (TFOpLambd (None, 5, 5, 448)    0           tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 448),  0           tf.nn.convolution_76[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_76 (TFOpLambda)      (None, 5, 5, 448)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_78 (TFOpLambd (None, 5, 5, 384)    0           tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_77 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_78[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_77[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_78 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_77 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_82 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_81 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_80 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_79 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_7 (TFO (None, 5, 5, 1280)   0           tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_84 (TFOpLambd (None, 5, 5, 320)    0           tf.concat_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_82[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_81[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_80[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_79[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_83 (TFOpLambd (None, 5, 5, 192)    0           tf.compat.v1.nn.avg_pool_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 320),  0           tf.nn.convolution_84[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_81 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_82 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_79 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_80 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 192),  0           tf.nn.convolution_83[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_83 (TFOpLambda)      (None, 5, 5, 320)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_9 (TFOpLambda)        (None, 5, 5, 768)    0           tf.nn.relu_81[0][0]              \n",
      "                                                                 tf.nn.relu_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_10 (TFOpLambda)       (None, 5, 5, 768)    0           tf.nn.relu_79[0][0]              \n",
      "                                                                 tf.nn.relu_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_84 (TFOpLambda)      (None, 5, 5, 192)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_11 (TFOpLambda)       (None, 5, 5, 2048)   0           tf.nn.relu_83[0][0]              \n",
      "                                                                 tf.concat_9[0][0]                \n",
      "                                                                 tf.concat_10[0][0]               \n",
      "                                                                 tf.nn.relu_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_85 (TFOpLambd (None, 5, 5, 448)    0           tf.concat_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 448),  0           tf.nn.convolution_85[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_85 (TFOpLambda)      (None, 5, 5, 448)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_87 (TFOpLambd (None, 5, 5, 384)    0           tf.concat_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_86 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_87[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_86[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_87 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_86 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_91 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_90 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_89 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_88 (TFOpLambd (None, 5, 5, 384)    0           tf.nn.relu_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.avg_pool_8 (TFO (None, 5, 5, 2048)   0           tf.concat_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_93 (TFOpLambd (None, 5, 5, 320)    0           tf.concat_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_91[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_90[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_89[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 384),  0           tf.nn.convolution_88[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.convolution_92 (TFOpLambd (None, 5, 5, 192)    0           tf.compat.v1.nn.avg_pool_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 320),  0           tf.nn.convolution_93[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_90 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_91 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_88 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_89 (TFOpLambda)      (None, 5, 5, 384)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.fused_batch_nor ((None, 5, 5, 192),  0           tf.nn.convolution_92[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_92 (TFOpLambda)      (None, 5, 5, 320)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_12 (TFOpLambda)       (None, 5, 5, 768)    0           tf.nn.relu_90[0][0]              \n",
      "                                                                 tf.nn.relu_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_13 (TFOpLambda)       (None, 5, 5, 768)    0           tf.nn.relu_88[0][0]              \n",
      "                                                                 tf.nn.relu_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.relu_93 (TFOpLambda)      (None, 5, 5, 192)    0           tf.compat.v1.nn.fused_batch_norm_\n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_14 (TFOpLambda)       (None, 5, 5, 2048)   0           tf.nn.relu_92[0][0]              \n",
      "                                                                 tf.concat_12[0][0]               \n",
      "                                                                 tf.concat_13[0][0]               \n",
      "                                                                 tf.nn.relu_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max (TFOpLambda) (None, 2048)         0           tf.concat_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           tf.math.reduce_max[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         2098176     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          524800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,623,489\n",
      "Trainable params: 2,623,489\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049832f6-2631-4b64-8ae7-2000bcfc4689",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Layer tf.nn.convolution was passed non-JSON-serializable arguments. Arguments had types: {'filters': <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, 'strides': [<class 'int'>, <class 'int'>], 'padding': <class 'str'>, 'dilations': [<class 'int'>, <class 'int'>], 'data_format': <class 'str'>, 'name': <class 'str'>}. They cannot be serialized out when saving the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/engine/node.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, make_node_key, node_conversion_map)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.4/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/saving/saved_model/json_utils.py\u001b[0m in \u001b[0;36mget_json_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ('Not JSON Serializable:', <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32, numpy=\narray([[[[-4.59105551e-01, -4.14526574e-02, -3.62577499e-03,\n          -9.87672508e-02, -3.37070860e-02,  4.79929037e-02,\n           2.32542142e-01,  3.23927671e-01,  5.90160079e-02,\n           9.47738215e-02,  4.24996056e-02,  1.26620471e-01,\n           1.33215979e-01,  1.22741833e-01, -7.92686343e-02,\n           2.08227970e-02, -1.99647829e-01, -3.02687973e-01,\n          -2.10659921e-01, -3.52898419e-01, -5.58023095e-01,\n           3.20223093e-01,  4.53596050e-03, -3.09266411e-02,\n          -6.86957911e-02,  2.00968519e-01,  1.14542745e-01,\n           2.40372881e-01,  1.52943470e-02,  5.96231259e-02,\n          -5.85714653e-02,  8.78173351e-01],\n         [ 3.16376328e-01, -4.78494260e-03, -6.84681814e-03,\n           7.44503662e-02, -1.97458476e-01,  5.20143658e-02,\n          -1.24460436e-01,  3.36123914e-01,  3.01678218e-02,\n          -1.26477107e-01, -1.05597451e-01, -5.31241074e-02,\n          -2.43102223e-01,  1.66047230e-01, -1.03186309e-01,\n          -6.55613691e-02,  2.60930099e-02,  1.16658449e-01,\n          -1.60814777e-01, -3.49688202e-01, -7.33594477e-01,\n           3.11471581e-01,  1.33009152e-02, -6.08066330e-03,\n          -7.95056075e-02, -3.26563753e-02,  4.55954410e-02,\n           2.36060649e-01,  5.62554598e-03,  6.51128590e-02,\n           1.04702845e-01, -1.66790619e-01],\n         [ 2.21712515e-01, -2.57035112e-03, -1.59502089e-01,\n          -2.51942836e-02,  2.43941799e-01,  2.55493037e-02,\n          -1.28795937e-01,  1.37704432e-01, -7.87055641e-02,\n           4.10648547e-02,  2.80119162e-02, -1.07536890e-01,\n           4.59116288e-02,  1.27241179e-01,  1.93367332e-01,\n           1.53290546e-02, -2.00637266e-01,  2.24361420e-01,\n          -1.29131392e-01, -9.51487944e-02, -3.70009631e-01,\n           2.87032187e-01, -8.66315290e-02,  9.28180665e-02,\n          -2.01310944e-02, -2.72079617e-01,  4.39127162e-02,\n          -7.55708218e-02, -9.42188203e-02, -5.93317747e-02,\n           3.96243595e-02, -6.19118512e-01]],\n\n        [[-7.51269162e-01,  3.48210037e-02, -7.98967332e-02,\n          -3.96422565e-01, -2.64644027e-01,  1.40673056e-01,\n          -5.92360973e-01,  5.70569396e-01, -5.61487041e-02,\n           1.59614719e-02,  1.91302866e-01, -3.66766155e-02,\n           3.75389487e-01,  3.75071824e-01, -1.85502082e-01,\n          -3.09550047e-01,  1.60525702e-02, -3.55118126e-01,\n          -2.88537711e-01, -9.48302820e-02,  1.53166577e-01,\n          -1.93490461e-01,  6.42974153e-02, -3.48205008e-02,\n          -1.65828705e-01,  3.90365124e-01,  1.26349302e-02,\n           1.53028890e-01,  1.33174499e-02,  7.45554641e-02,\n          -1.92786697e-02,  1.75148576e-01],\n         [ 3.55816573e-01, -3.14642340e-02,  1.28206044e-01,\n          -1.28941059e-01, -5.46135902e-02,  2.78681666e-01,\n           4.55734700e-01,  5.38976073e-01,  8.31630006e-02,\n          -2.80416071e-01, -1.48861542e-01,  1.21901974e-01,\n          -3.83083135e-01,  3.75166088e-01, -2.08227515e-01,\n          -1.29338771e-01,  3.50710601e-01,  2.24829260e-02,\n          -2.44429782e-01, -6.89486489e-02,  1.19071797e-01,\n          -9.07485932e-02,  6.37152195e-02, -6.23455318e-03,\n          -1.83708176e-01,  3.16314101e-02, -3.26027162e-02,\n           1.46927759e-01,  5.60333719e-03,  1.00134099e-02,\n           1.34500518e-01, -9.49547514e-02],\n         [ 5.11001050e-01,  8.04265440e-02,  9.84589979e-02,\n          -9.28192511e-02,  4.39067900e-01,  2.40116969e-01,\n           1.48012802e-01,  2.06206009e-01, -3.86097394e-02,\n           2.34553993e-01,  2.01258138e-02, -1.33050963e-01,\n           1.42953947e-01,  1.52850181e-01,  3.48664939e-01,\n          -2.21172974e-01, -1.68190915e-02,  2.35393152e-01,\n          -1.90555871e-01, -3.70343146e-03,  1.04658179e-01,\n          -3.13451141e-02,  8.90399516e-02,  4.66634482e-02,\n          -7.73299858e-02, -3.79417181e-01, -4.08192798e-02,\n          -4.52801585e-01, -7.00017810e-02, -9.71016958e-02,\n           2.95030206e-01, -1.05120182e-01]],\n\n        [[-7.53074363e-02, -2.06811968e-02,  8.94671679e-02,\n           1.55051500e-02, -7.04864040e-02, -9.22501162e-02,\n          -7.11003184e-01,  2.50318170e-01,  1.14692561e-01,\n           9.74843428e-02,  8.26721266e-02, -6.69363514e-02,\n           1.83123484e-01, -2.55230129e-01, -1.04877211e-01,\n           8.40508714e-02, -1.46275967e-01, -3.01192343e-01,\n          -1.65155828e-01, -1.95000917e-02,  2.92843819e-01,\n          -2.51484841e-01,  8.99823979e-02, -5.07941581e-02,\n          -1.54783919e-01,  3.13909143e-01,  4.67576571e-02,\n           9.33493748e-02, -8.23359098e-03,  1.06634155e-01,\n          -3.46046425e-02, -3.19266319e-01],\n         [ 1.12931758e-01, -1.48703828e-02,  6.78641945e-02,\n           1.50543079e-01, -1.13253519e-01,  8.58298391e-02,\n           5.01982749e-01,  1.83798179e-01, -8.68951753e-02,\n          -1.50101945e-01, -1.27019748e-01,  2.00995386e-01,\n          -1.94146752e-01, -3.33526462e-01, -3.95116657e-02,\n          -3.76722105e-02,  8.60670023e-03, -6.98672514e-03,\n          -1.00546576e-01, -2.62953830e-03,  4.65099514e-01,\n          -2.14414850e-01,  7.64784589e-02, -5.08379657e-03,\n          -1.73704341e-01, -8.37199613e-02, -4.17399816e-02,\n           1.73426822e-01,  2.95095835e-02,  6.04446344e-02,\n           9.06928107e-02, -1.14271380e-02],\n         [-7.29609355e-02,  3.49589698e-02, -1.79962412e-01,\n           1.22079901e-01,  1.76442862e-01,  1.16373926e-01,\n           2.30988681e-01,  3.72244976e-02, -3.20714563e-02,\n           5.88952675e-02, -3.42379771e-02, -9.31739733e-02,\n          -1.16255339e-02, -1.73515990e-01,  1.21214695e-01,\n           1.40511736e-01, -2.19165638e-01,  1.94948763e-01,\n          -9.93858278e-02, -1.37543278e-02,  2.88932323e-01,\n          -2.54598916e-01,  8.18339437e-02,  1.16559630e-02,\n          -9.36568975e-02, -2.18580514e-01, -1.22934952e-01,\n           5.50779048e-04, -7.01512843e-02, -3.58531252e-02,\n           1.04194552e-01,  2.88285494e-01]]],\n\n\n       [[[-3.09671372e-01,  2.32201777e-02, -1.96793169e-01,\n          -2.20510378e-01, -2.42669106e-01,  4.67890911e-02,\n           2.48323604e-01, -1.66484311e-01, -3.84278834e-01,\n           3.80987860e-02, -4.54362422e-01, -2.37979427e-01,\n           3.29888821e-01,  3.41433752e-03, -8.28291476e-02,\n           3.99100259e-02, -2.70952880e-01, -3.21505904e-01,\n           1.00008205e-01,  5.77513278e-01, -4.18643892e-01,\n           2.54249245e-01,  5.62861599e-02, -8.69987980e-02,\n          -6.83216453e-02,  3.79922688e-01,  1.76942736e-01,\n           2.11315155e-01,  6.92530796e-02, -1.69373155e-02,\n          -2.29212847e-02,  4.36451226e-01],\n         [ 1.85230255e-01,  1.21470345e-02,  1.15113854e-01,\n           9.49417576e-02, -1.77649230e-01,  1.30133405e-01,\n          -1.27454177e-01, -1.52578086e-01,  4.32037741e-01,\n          -2.64863998e-01, -3.03227827e-02,  2.39914760e-01,\n          -4.59962994e-01,  3.57799567e-02, -1.57588765e-01,\n          -5.69069721e-02,  4.98269275e-02,  7.87699968e-02,\n           9.30080563e-02,  6.36520803e-01, -5.54792702e-01,\n           1.93763480e-01,  1.40769668e-02, -7.37030208e-02,\n          -7.34341666e-02,  4.33748625e-02,  1.17953978e-01,\n           3.15200478e-01,  4.22935151e-02, -3.14331055e-03,\n           6.22104593e-02, -8.47976282e-02],\n         [ 1.60247013e-01,  5.20873033e-02,  1.04668580e-01,\n           6.11611344e-02,  3.69409770e-01,  4.96275760e-02,\n          -1.12423740e-01, -5.49178086e-02, -9.42278504e-02,\n           2.54325986e-01, -4.13073972e-02,  6.60333559e-02,\n           1.11361548e-01,  3.90531085e-02,  2.56812125e-01,\n           3.07825580e-02, -3.65221053e-01,  2.65386939e-01,\n           8.95922631e-02,  2.37859935e-01, -2.19269782e-01,\n           1.70323789e-01, -2.40444541e-02, -6.42235950e-02,\n          -5.73639609e-02, -3.37285519e-01,  2.47313276e-01,\n          -2.12214038e-01, -1.53777460e-02, -1.04467109e-01,\n          -1.30376607e-01, -2.94386506e-01]],\n\n        [[-5.50010741e-01,  1.43946797e-01, -5.10051787e-01,\n          -4.68869209e-01, -5.08882999e-01, -1.71292290e-01,\n          -1.13478772e-01, -5.71783721e-01, -3.67561132e-01,\n          -1.21380776e-01, -1.13262475e-01, -6.48380578e-01,\n           6.27203345e-01,  6.94373548e-01, -1.95106730e-01,\n          -2.64451325e-01, -2.00940773e-01, -4.21104223e-01,\n           3.08264852e-01,  8.29216838e-03,  3.95820379e-01,\n          -3.35089803e-01,  9.52083990e-02,  2.36721095e-02,\n          -1.13931984e-01,  6.35013640e-01,  6.06294833e-02,\n           4.41855192e-01, -4.77887504e-02, -1.98351786e-01,\n          -1.94868110e-02, -4.85313162e-02],\n         [ 1.90947160e-01,  4.33590598e-02,  4.24152821e-01,\n          -3.69650614e-03,  5.56460768e-03,  6.86372221e-02,\n           1.50262251e-01, -5.79318464e-01,  3.83384645e-01,\n          -3.96539152e-01, -2.05095951e-02,  5.08662283e-01,\n          -6.81817293e-01,  6.39929175e-01, -2.63347208e-01,\n          -1.64095819e-01,  1.31546974e-01,  7.51749100e-03,\n           2.77786314e-01,  1.27530554e-02,  4.76832241e-01,\n          -2.03199059e-01,  3.82504165e-02,  4.77481186e-02,\n          -1.21142201e-01,  1.24962404e-01,  1.94157716e-02,\n           4.35586035e-01, -5.81540316e-02, -2.21681744e-01,\n           9.22068208e-02, -1.54824508e-02],\n         [ 3.69268328e-01,  1.97107822e-01,  6.14967883e-01,\n           1.44848540e-01,  6.79024100e-01,  8.71696472e-02,\n          -2.52940003e-02, -2.09089696e-01, -7.00887442e-02,\n           5.60278237e-01,  5.41306697e-02,  1.50142446e-01,\n           1.22068822e-01,  2.51376867e-01,  4.28028524e-01,\n          -1.99277475e-01, -3.64807039e-01,  3.26477617e-01,\n           2.77842641e-01,  5.99431852e-03,  2.13500857e-01,\n          -7.79086575e-02,  1.37640312e-01,  1.45664038e-02,\n          -7.50476941e-02, -6.03472531e-01,  9.13274661e-02,\n          -5.51822066e-01, -4.93564568e-02, -2.41208330e-01,\n           1.42374914e-02,  7.20060058e-03]],\n\n        [[-6.13205209e-02,  1.67170428e-02, -2.21705899e-01,\n          -4.95959334e-02, -2.37929299e-01, -5.35335004e-01,\n          -2.11950779e-01, -1.99938426e-03,  8.72384310e-02,\n          -7.92514980e-02,  3.13561596e-02, -5.00510335e-01,\n           3.16275716e-01, -1.79683864e-01, -7.73780569e-02,\n           1.09483354e-01, -3.62387270e-01, -3.29678863e-01,\n           9.10189226e-02, -6.27809390e-02,  3.97496015e-01,\n          -2.75063694e-01,  9.81727690e-02, -9.63142794e-03,\n          -1.43188626e-01,  3.90550435e-01,  6.00194223e-02,\n           3.73257279e-01,  7.54999146e-02, -2.31349897e-02,\n          -7.19401911e-02, -3.81740510e-01],\n         [ 4.15264070e-02,  1.05704041e-02,  2.22087592e-01,\n           2.28352740e-01, -8.31382945e-02, -2.79769450e-01,\n           1.47461593e-01, -9.23275203e-03, -6.49965033e-02,\n          -2.37697676e-01, -1.14193618e-01,  4.09631103e-01,\n          -4.53702658e-01, -2.49112755e-01, -8.12669992e-02,\n          -3.03775985e-02, -1.21517688e-01,  3.44517045e-02,\n           8.81673172e-02, -8.83365348e-02,  5.15151560e-01,\n          -2.08372459e-01,  3.21709625e-02,  5.33515327e-02,\n          -1.59795165e-01, -8.11125860e-02, -1.20032309e-02,\n           4.29726124e-01,  9.70903337e-02, -6.00864813e-02,\n           3.75584215e-02,  5.58782928e-02],\n         [-2.62930449e-02,  1.01349048e-01,  2.71942019e-01,\n           2.09343761e-01,  3.21388841e-01, -1.02254741e-01,\n           3.66898850e-02,  2.09949017e-02, -6.04890622e-02,\n           3.25812578e-01, -2.36350242e-02,  9.74250138e-02,\n          -3.49432509e-03, -1.29163429e-01,  1.58048660e-01,\n           9.56366658e-02, -4.31959033e-01,  3.28082532e-01,\n           3.51306573e-02, -6.12014011e-02,  2.15223819e-01,\n          -1.76527336e-01,  9.22026634e-02,  2.45687384e-02,\n          -1.40548587e-01, -4.53587860e-01, -9.22924131e-02,\n          -1.26142427e-01, -4.48163524e-02, -1.39643848e-01,\n          -8.38275552e-02,  2.74723858e-01]]],\n\n\n       [[[ 1.61150634e-01, -2.86575016e-02,  7.11555034e-02,\n          -2.07143471e-01,  5.29730059e-02,  9.41791460e-02,\n           2.55565137e-01,  7.62317255e-02, -5.32264113e-02,\n           1.34586126e-01, -9.10150483e-02, -1.65695250e-01,\n           2.42919326e-01, -2.00146437e-01,  1.74079668e-02,\n           7.50095099e-02,  1.55196548e-01, -2.58261949e-01,\n          -4.92655626e-03,  4.16197777e-01, -3.86153311e-02,\n           1.46446452e-01,  5.06453626e-02, -3.60621393e-01,\n           1.15070350e-01,  2.89169192e-01,  1.00484222e-01,\n          -1.09357260e-01, -7.28665618e-03,  4.20598984e-02,\n          -3.83202098e-02, -1.05859242e-01],\n         [ 6.39319569e-02, -3.89898079e-03,  4.97516058e-03,\n          -6.25403598e-02, -2.42724687e-01,  2.47927323e-01,\n          -1.59898847e-01,  4.75750826e-02,  1.23555377e-01,\n          -1.89252675e-01,  1.06582632e-02,  1.46695763e-01,\n          -2.47447804e-01, -2.17797235e-01, -5.32606617e-02,\n          -1.12605102e-01,  3.69681597e-01,  6.58257976e-02,\n          -9.96394455e-03,  4.14005488e-01,  5.81203122e-03,\n          -2.15347242e-02, -1.33059714e-02, -2.35672444e-01,\n           1.47929698e-01, -9.68965814e-02, -3.85954790e-02,\n           3.19409184e-02, -2.63723861e-02,  1.10327229e-01,\n           8.80503729e-02, -5.33311628e-02],\n         [-2.27521077e-01,  3.65342870e-02, -1.42364472e-01,\n          -1.69696268e-02,  1.87642574e-01,  1.41795829e-01,\n          -1.04727075e-01, -3.72307226e-02, -7.86822960e-02,\n           5.50233088e-02,  4.85404283e-02, -1.27198761e-02,\n           4.12139669e-02, -8.29386860e-02,  2.39679087e-02,\n           7.99235627e-02, -1.16620973e-01,  2.00591236e-01,\n          -1.69659853e-02,  1.10958219e-01,  6.90503558e-03,\n           2.02338211e-02, -3.25619727e-02, -2.45006755e-01,\n           3.24574076e-02, -2.77086586e-01,  9.08047485e-04,\n          -4.50409018e-03, -7.13055953e-02, -5.91237992e-02,\n          -1.74784988e-01,  8.49652290e-02]],\n\n        [[ 4.06208187e-02, -1.34799313e-02, -2.80702084e-01,\n          -4.87784952e-01, -2.33685091e-01,  1.71019688e-01,\n           2.52302170e-01, -5.00181317e-01, -8.71177763e-02,\n          -2.00075116e-02,  3.82897705e-02, -4.43741232e-01,\n           3.89617324e-01, -2.45864466e-01, -4.76957001e-02,\n           1.28133371e-01,  3.14029723e-01, -3.26389074e-01,\n           1.34655774e-01, -3.44568610e-01, -1.75894618e-01,\n           1.71287939e-01,  5.22936471e-02, -2.89032217e-02,\n           2.87097365e-01,  4.60938752e-01,  5.09101227e-02,\n          -4.64511588e-02, -2.71178037e-01, -6.56533614e-02,\n           5.03007583e-02, -1.58027932e-01],\n         [ 3.31376269e-02, -1.52911358e-02,  6.23353198e-02,\n          -1.69642091e-01, -1.95648819e-01,  3.87342900e-01,\n          -1.82435453e-01, -4.02341664e-01,  9.91701782e-02,\n          -2.99883604e-01, -1.95305813e-02,  3.51800621e-01,\n          -4.53105867e-01, -2.41416395e-01, -1.03277519e-01,\n          -7.88784996e-02,  4.33811039e-01,  7.86142126e-02,\n           4.23696041e-02, -3.05828214e-01, -1.67982697e-01,\n           1.13012172e-01, -1.87792554e-02,  5.92934899e-02,\n           3.61771584e-01, -4.57345620e-02, -3.06100622e-02,\n          -3.73655446e-02, -2.49070913e-01, -4.13494818e-02,\n           2.06367165e-01, -6.83298660e-03],\n         [-5.36594018e-02,  9.36179534e-02,  1.65238991e-01,\n           5.53405955e-02,  2.57401228e-01,  2.50335783e-01,\n          -6.45859763e-02, -1.18017726e-01, -7.41348714e-02,\n           2.81930178e-01,  6.65699691e-02,  5.19486517e-02,\n           4.24872227e-02, -1.16484366e-01,  1.29901171e-01,\n           7.61362910e-02, -2.09316850e-01,  2.89463013e-01,\n           1.01264454e-01, -1.38873145e-01, -1.44799188e-01,\n           6.74227998e-02, -1.34168856e-03,  3.55463102e-02,\n           2.26262435e-01, -5.01563668e-01, -4.28435691e-02,\n          -2.84834534e-01, -1.30743161e-01, -1.03075445e-01,\n          -9.36047584e-02,  1.46130726e-01]],\n\n        [[ 8.36313888e-02, -8.29154029e-02, -6.68296069e-02,\n          -3.35510015e-01,  8.31014588e-02, -1.64307915e-02,\n           2.61760324e-01, -3.64583619e-02,  1.95155665e-01,\n           3.21608484e-02,  8.42251927e-02, -2.32356116e-01,\n           2.78860003e-01, -2.21362099e-01, -5.41272573e-03,\n           4.74610850e-02,  1.59989282e-01, -2.72452205e-01,\n           9.41323563e-02, -1.26402825e-01, -1.48332641e-02,\n           9.81647000e-02,  5.77408485e-02, -6.89573810e-02,\n           2.52074748e-01,  4.18857992e-01,  8.16523656e-02,\n          -9.57819726e-03, -1.64164416e-02,  4.02872637e-02,\n          -2.40528770e-02, -1.29585251e-01],\n         [ 1.14373878e-01, -4.22538212e-03,  7.03023151e-02,\n          -1.17636502e-01, -1.59701794e-01,  1.23792440e-01,\n          -1.90807760e-01, -2.16314141e-02, -1.32634625e-01,\n          -1.28451735e-01, -7.51707479e-02,  2.57721603e-01,\n          -2.11595684e-01, -1.25269800e-01,  1.99442990e-02,\n          -4.90545258e-02,  2.44563296e-01,  9.58911777e-02,\n           5.96616194e-02, -1.61616415e-01, -1.09070748e-01,\n          -1.20989429e-02, -7.77932117e-03,  6.38317689e-02,\n           3.04483294e-01, -2.50075106e-02, -1.87520199e-02,\n          -7.08285794e-02,  3.56728099e-02,  7.37871975e-02,\n           1.14570551e-01, -6.01681806e-02],\n         [-1.94149122e-01,  6.73346743e-02,  6.54700622e-02,\n           5.41231073e-02,  9.72744748e-02,  8.48492980e-02,\n          -8.35040435e-02, -3.32129709e-02, -1.16422340e-01,\n           1.00631602e-01,  3.66899348e-03,  1.04717100e-02,\n          -3.40034589e-02, -2.21335012e-02, -2.93729398e-02,\n           3.90057638e-02, -1.44896954e-01,  2.96015412e-01,\n          -2.27687247e-02, -9.77180004e-02, -8.71916488e-02,\n          -7.66254216e-02, -3.22300009e-02,  1.45472540e-02,\n           1.66192874e-01, -3.18722308e-01, -8.46264958e-02,\n          -1.36219218e-01, -5.61135188e-02, -4.55730930e-02,\n          -8.21031332e-02,  1.48212612e-01]]]], dtype=float32)>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ef0fa6b69d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2084\u001b[0m     \"\"\"\n\u001b[1;32m   2085\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2086\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   2087\u001b[0m                     signatures, options, save_traces)\n\u001b[1;32m   2088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0;34m'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m--> 146\u001b[0;31m     hdf5_format.save_model_to_hdf5(\n\u001b[0m\u001b[1;32m    147\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[1;32m   1345\u001b[0m           \u001b[0;31m# The node is relevant to the model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;31m# add to filtered_inbound_nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m           \u001b[0mnode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_node_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_conversion_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m           \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/user/Doctoraizer/.venv/lib/python3.9/site-packages/keras/engine/node.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, make_node_key, node_conversion_map)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m       \u001b[0mkwarg_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       raise TypeError('Layer ' + self.layer.name +\n\u001b[0m\u001b[1;32m    190\u001b[0m                       \u001b[0;34m' was passed non-JSON-serializable arguments. '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                       \u001b[0;34m'Arguments had types: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Layer tf.nn.convolution was passed non-JSON-serializable arguments. Arguments had types: {'filters': <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, 'strides': [<class 'int'>, <class 'int'>], 'padding': <class 'str'>, 'dilations': [<class 'int'>, <class 'int'>], 'data_format': <class 'str'>, 'name': <class 'str'>}. They cannot be serialized out when saving the model."
     ]
    }
   ],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b29131-599a-495d-9e7e-23aa83dad61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch=train_generator.samples//batch_size,\n",
    "          epochs = 10,\n",
    "          validation_data=valid_generator,\n",
    "          validation_steps=valid_generator.samples//batch_size)\n",
    "\n",
    "\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.00001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "temp = valid_generator.samples//batch_size\n",
    "\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=train_generator.samples//batch_size,\n",
    "          epochs = 5,\n",
    "          validation_data=valid_generator,\n",
    "          validation_steps=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c26df-017e-4223-8050-4a5ca6199622",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_path,\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90655e-3a04-4958-90df-70cb2e7d931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9be134-19e8-45f5-bd7e-d88885956483",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_generator.classes, y_pred=np.argmax(train_generator, axis=-1))\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "test_generator.class_indices\n",
    "cm_plot_labels = ['n', 'p']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404cb9b-7c80-4663-85d9-86e99a34715c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a436666-fdbc-47f0-bd49-e53a406008a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e11076-d7d0-47b6-8d82-d1ea7884ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
